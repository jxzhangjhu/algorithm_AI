{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c95482f6",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/machine-learning-basics-with-the-k-nearest-neighbors-algorithm-6a6e71d01761\n",
    "这个帖子写的特别好！ \n",
    "\n",
    "\n",
    "The KNN Algorithm\n",
    "\n",
    "1. Load the data\n",
    "2. Initialize K to your chosen number of neighbors\n",
    "3. For each example in the data\n",
    "\n",
    "    3.1 Calculate the distance between the query example and the current example from the data.\n",
    "    \n",
    "    3.2 Add the distance and the index of the example to an ordered collection\n",
    "    \n",
    "    \n",
    "4. Sort the ordered collection of distances and indices from smallest to largest (in ascending order) by the distances\n",
    "\n",
    "5. Pick the first K entries from the sorted collection\n",
    "\n",
    "6. Get the labels of the selected K entries\n",
    "\n",
    "7. If regression, return the mean of the K labels\n",
    "\n",
    "8. If classification, return the mode of the K labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbcffc6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reg_prediction 128.24666666666667\n",
      "clf_prediction 0\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "def knn(data, query, k, distance_fn, choice_fn):\n",
    "    neighbor_distances_and_indices = []\n",
    "    \n",
    "    # 3. For each example in the data\n",
    "    for index, example in enumerate(data):\n",
    "        # 3.1 Calculate the distance between the query example and the current\n",
    "        # example from the data.\n",
    "        distance = distance_fn(example[:-1], query)\n",
    "        \n",
    "        # 3.2 Add the distance and the index of the example to an ordered collection\n",
    "        neighbor_distances_and_indices.append((distance, index))\n",
    "    \n",
    "    # 4. Sort the ordered collection of distances and indices from\n",
    "    # smallest to largest (in ascending order) by the distances\n",
    "    sorted_neighbor_distances_and_indices = sorted(neighbor_distances_and_indices)\n",
    "    \n",
    "    # 5. Pick the first K entries from the sorted collection\n",
    "    k_nearest_distances_and_indices = sorted_neighbor_distances_and_indices[:k]\n",
    "    \n",
    "    # 6. Get the labels of the selected K entries\n",
    "    k_nearest_labels = [data[i][-1] for distance, i in k_nearest_distances_and_indices]\n",
    "\n",
    "    # 7. If regression (choice_fn = mean), return the average of the K labels\n",
    "    # 8. If classification (choice_fn = mode), return the mode of the K labels\n",
    "    return k_nearest_distances_and_indices , choice_fn(k_nearest_labels)\n",
    "\n",
    "def mean(labels):\n",
    "    return sum(labels) / len(labels)\n",
    "\n",
    "def mode(labels):\n",
    "    return Counter(labels).most_common(1)[0][0]\n",
    "\n",
    "def euclidean_distance(point1, point2):\n",
    "    sum_squared_distance = 0\n",
    "    for i in range(len(point1)):\n",
    "        sum_squared_distance += math.pow(point1[i] - point2[i], 2)\n",
    "    return math.sqrt(sum_squared_distance)\n",
    "\n",
    "def main():\n",
    "    '''\n",
    "    # Regression Data\n",
    "    # \n",
    "    # Column 0: height (inches)\n",
    "    # Column 1: weight (pounds)\n",
    "    '''\n",
    "    reg_data = [\n",
    "       [65.75, 112.99],\n",
    "       [71.52, 136.49],\n",
    "       [69.40, 153.03],\n",
    "       [68.22, 142.34],\n",
    "       [67.79, 144.30],\n",
    "       [68.70, 123.30],\n",
    "       [69.80, 141.49],\n",
    "       [70.01, 136.46],\n",
    "       [67.90, 112.37],\n",
    "       [66.49, 127.45],\n",
    "    ]\n",
    "    \n",
    "    # Question:\n",
    "    # Given the data we have, what's the best-guess at someone's weight if they are 60 inches tall?\n",
    "    reg_query = [60]\n",
    "    reg_k_nearest_neighbors, reg_prediction = knn(\n",
    "        reg_data, reg_query, k=3, distance_fn=euclidean_distance, choice_fn=mean\n",
    "    )\n",
    "    \n",
    "    print('reg_prediction',reg_prediction)\n",
    "    \n",
    "    '''\n",
    "    # Classification Data\n",
    "    # \n",
    "    # Column 0: age\n",
    "    # Column 1: likes pineapple\n",
    "    '''\n",
    "    clf_data = [\n",
    "       [22, 1],\n",
    "       [23, 1],\n",
    "       [21, 1],\n",
    "       [18, 1],\n",
    "       [19, 1],\n",
    "       [25, 0],\n",
    "       [27, 0],\n",
    "       [29, 0],\n",
    "       [31, 0],\n",
    "       [45, 0],\n",
    "    ]\n",
    "    # Question:\n",
    "    # Given the data we have, does a 33 year old like pineapples on their pizza?\n",
    "    clf_query = [33]\n",
    "    clf_k_nearest_neighbors, clf_prediction = knn(\n",
    "        clf_data, clf_query, k=3, distance_fn=euclidean_distance, choice_fn=mode\n",
    "    )\n",
    "    \n",
    "    print('clf_prediction', clf_prediction)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9623c7",
   "metadata": {},
   "source": [
    "# Choosing the right value for K\n",
    "To select the K that’s right for your data, we run the KNN algorithm several times with different values of K and choose the K that reduces the number of errors we encounter while maintaining the algorithm’s ability to accurately make predictions when it’s given data it hasn’t seen before.\n",
    "\n",
    "Here are some things to keep in mind:\n",
    "\n",
    "1. As we decrease the value of K to 1, our predictions become less stable. Just think for a minute, imagine K=1 and we have a query point surrounded by several reds and one green (I’m thinking about the top left corner of the colored plot above), but the green is the single nearest neighbor. Reasonably, we would think the query point is most likely red, but because K=1, KNN incorrectly predicts that the query point is green.\n",
    "\n",
    "\n",
    "2. Inversely, as we increase the value of K, our predictions become more stable due to majority voting / averaging, and thus, more likely to make more accurate predictions (up to a certain point). Eventually, we begin to witness an increasing number of errors. It is at this point we know we have pushed the value of K too far.\n",
    "\n",
    "\n",
    "3. In cases where we are taking a majority vote (e.g. picking the mode in a classification problem) among labels, we usually make K an odd number to have a tiebreaker."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3199211",
   "metadata": {},
   "source": [
    "# Advantages\n",
    "1. The algorithm is simple and easy to implement.\n",
    "2. There’s no need to build a model, tune several parameters, or make additional assumptions.\n",
    "3. The algorithm is versatile. It can be used for classification, regression, and search (as we will see in the next section).\n",
    "\n",
    "# Disadvantages\n",
    "1. The algorithm gets significantly slower as the number of examples and/or predictors/independent variables increase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925f6860",
   "metadata": {},
   "source": [
    "# Recommender systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9c46e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 Years a Slave\n",
      "Hacksaw Ridge\n",
      "Queen of Katwe\n",
      "The Wind Rises\n",
      "A Beautiful Mind\n"
     ]
    }
   ],
   "source": [
    "# from knn_from_scratch import knn, euclidean_distance\n",
    "\n",
    "def recommend_movies(movie_query, k_recommendations):\n",
    "    raw_movies_data = []\n",
    "    with open('movies_recommendation_data.csv', 'r') as md:\n",
    "        # Discard the first line (headings)\n",
    "        next(md)\n",
    "\n",
    "        # Read the data into memory\n",
    "        for line in md.readlines():\n",
    "            data_row = line.strip().split(',')\n",
    "            raw_movies_data.append(data_row)\n",
    "\n",
    "    # Prepare the data for use in the knn algorithm by picking\n",
    "    # the relevant columns and converting the numeric columns\n",
    "    # to numbers since they were read in as strings\n",
    "    movies_recommendation_data = []\n",
    "    for row in raw_movies_data:\n",
    "        data_row = list(map(float, row[2:]))\n",
    "        movies_recommendation_data.append(data_row)\n",
    "\n",
    "    # Use the KNN algorithm to get the 5 movies that are most\n",
    "    # similar to The Post.\n",
    "    recommendation_indices, _ = knn(\n",
    "        movies_recommendation_data, movie_query, k=k_recommendations,\n",
    "        distance_fn=euclidean_distance, choice_fn=lambda x: None\n",
    "    )\n",
    "\n",
    "    movie_recommendations = []\n",
    "    for _, index in recommendation_indices:\n",
    "        movie_recommendations.append(raw_movies_data[index])\n",
    "\n",
    "    return movie_recommendations\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    the_post = [7.2, 1, 1, 0, 0, 0, 0, 1, 0] # feature vector for The Post\n",
    "    recommended_movies = recommend_movies(movie_query=the_post, k_recommendations=5)\n",
    "\n",
    "    # Print recommended movie titles\n",
    "    for recommendation in recommended_movies:\n",
    "        print(recommendation[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73b8fd3",
   "metadata": {},
   "source": [
    "# Summary \n",
    "\n",
    "The k-nearest neighbors (KNN) algorithm is a simple, supervised machine learning algorithm that can be used to solve both classification and regression problems. It’s easy to implement and understand, but has a major drawback of becoming significantly slows as the size of that data in use grows.\n",
    "\n",
    "KNN works by finding the distances between a query and all the examples in the data, selecting the specified number examples (K) closest to the query, then votes for the most frequent label (in the case of classification) or averages the labels (in the case of regression).\n",
    "\n",
    "In the case of classification and regression, we saw that choosing the right K for our data is done by trying several Ks and picking the one that works best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235a50e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
